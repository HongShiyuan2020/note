### 思考

1. 思考其他领域的应用（详细阐述任务）。
	1. 具有难度的数学题目推理过程。
2. 分析LoT的不足，该工作未考虑的任务。

### 问题

思维链（Chain-of-Thought, CoT）引导大型语言模型（LLMs）逐步推理，从而激发其逻辑推理能力。尽管该方法在逻辑任务中效果显著，但CoT 并不利于需要跳出常规思维的创造性问题解决，而这种能力对于创新发展至关重要。

本文探索 LLMs 的跳跃思维（Leap-of-Thought, LoT）能力——这是一种非顺序性的、具有**强关联性和知识飞跃**的创造性范式。为此，本文基于广受欢迎的“大喜利”（Oogiri）游戏对 LLMs 进行研究，该游戏要求参与者具有出色的创造力和强大的联想能力，以便对给定的图像、文本或情境做出意想不到且幽默的回应。

![[Pasted image 20241119123939.png]]

### 本文贡献

1. 构建了一个多模态、多语言的 Oogiri-GO 数据集，该数据集包含来自“大喜利”游戏的 13 万多个样本。
	1. ![[Pasted image 20241119124519.png]]
2. 提出了一种创造性跃式思维（CLoT）范式，以提升 LLM 的跃式思维（LoT）能力。
	1. ![[Pasted image 20241119124857.png]]
	2. ![[Pasted image 20241119124817.png]]
	3. CLoT 首先将 Oogiri-GO 数据集整理为面向 LoT 的指令调优数据，用于训练预训练的 LLM，从而实现一定程度的 LoT 幽默生成和判别能力。（Associable Instruction Tuning）
	4. CLoT 设计了一种探索性的自我优化机制，通过挖掘看似不相关概念之间的类比，鼓励 LLM 生成更具创造性的 LoT 数据，并从中筛选高质量数据进行训练，以实现自我优化。（Explorative Self-Refinement）

### Associable Instruction Tuning

跳跃思维（LoT）能力主要包括关联生成能力和关联判别能力 。
**关联生成**通过远程联想发现与看似无关的概念之间的类比，从而生成创新性的响应，例如为“大喜利”输入提供出人意料的幽默回答。
**关联判别**则是评估输入与响应之间的匹配程度，即使它们表面上似乎无关，从中选择最具创造性的响应。

#### Instruction Generation & Discrimination Templates

![[Pasted image 20241119124954.png]]
![[Pasted image 20241119125008.png]]

#### Associable Instruction Learning

通过使用上述指令模板，本文将 Oogiri-GO 数据集中的 13 万个样本扩充为超过 50 万条指令。在训练过程中，要求 LLM 根据包含“任务特定提示”（Task-specific Prompt）以及两个可选条件（如图像条件和文本条件）的“用户输入”（USER-INPUTs）来预测“任务特定响应”（task-specific responses）。为了避免过拟合，本文仅使用可关联的指令数据对 LLM 的标准 LoRA 进行训练。

### Explorative Self-Refinement

在完成关联指令调优后，我们旨在通过 LLM 生成更多高质量的创造性数据，并利用这些数据对 LLM 进行训练以实现自我优化。

#### Explorative Remote Association

核心在于引导 LLM 在弱关联条件下生成多样化的创造性响应。为实现这一点，我们从 Oogiri-GO 训练数据的文本中提取了一组名词对象，记作集合 $S$（详细信息见附录）。随后，对于每个用户输入 ，我们生成 nnn 个弱关联条件 ${Ci}i=1n\{C_i\}_{i=1}^n{Ci​}i=1n​$。这些条件可以以概率 ρ∈(0,1)\rho \in (0, 1)ρ∈(0,1) 为空，从而赋予 LLM 自由生成的空间，或者从名词集合 SSS 中均匀随机抽样，强制 LLM 在不同概念之间建立联系。接着，我们将条件 CiC_iCi​ 添加到用户输入 III 中，并将其输入到 LLM 中以生成一个幽默候选项 RiR_iRi​。通过对不同的条件 CiC_iCi​ 重复此过程，可以生成总计 nnn 个候选项 {Ri}i=1n\{R_i\}_{i=1}^n{Ri​}i=1n​。

1. 从 Oogiri-GO 训练数据的文本中提取了一组名词对象，记作集合 $S$。
2. 对于每个用户输入 $I$，生成 $n$ 个弱关联条件 $\{C_i\}_{i=1}^n$。这些条件可以以概率 $ρ∈(0,1)$为空，从而赋予 LLM 自由生成的空间，或者从名词集合 $S$ 中均匀随机抽样，强制 LLM 在不同概念之间建立联系。
3. 将条件 $Ci$​ 添加到用户输入 $I$ 中，并将其输入到 LLM 中以生成一个幽默候选项 $R_i$​。通过对不同的条件 $C_i$​ 重复此过程，可以生成总计 $n$ 个候选项 $\{R_i\}_{i=1}^n$​。
4. LLM 利用其在之前学习的**判别排序能力**对这些候选项进行排序。
5. 将排名前两位的候选项与真实答案（GTR）混合，并从中选择排名第一的作为最终响应。最后，如果选出的排名第一的响应是 GTR，则丢弃该样本。通过重复这一过程，逐步收集到足够的新高质量数据。

#### Self-refinement

将上述生成的指令与之前的基础指令调优样本相结合，形成一个包含超过 55 万个样本的数据集，再次用于训练我们的 LLM。

### CLoT Inference

![[Pasted image 20241119130538.png]]

### Result

![[Pasted image 20241119130705.png]]

![[Pasted image 20241119130746.png]]

![[Pasted image 20241119130753.png]]
![[Pasted image 20241119130811.png]]


$$
	\begin{matrix}
		a & b \\
		c & d \\
	\end{matrix}
$$

$$
$$

