## 问题

- 面向电路连接的视觉感知
	- 电路连接场景的独特性
		- 1，2，3 ....
	- 如何针对独特性涉及解决方案
 
**图像特征和文本特征的深度融合**&**保持LLM的原始自然语言处理性能**
### 图像文本融合方法

训练一个大型语言模型本身已经十分不易，而从零开始训练一个视觉语言模型（VLM）并使其具备与经过充分训练的纯语言模型（如 LLaMA2，Touvron 等，2023）**相同的自然语言处理性能**，则更具挑战性。
1. **浅层对齐方法**：浅层对齐方法通常将**图像特征映射到语言模型的输入空间**。（InstructBLIP、MiniGPT-4，借助Q-Former后线性层），这种方法**收敛速度较快**，其性能明显不如具有**可训练语言参数的 LLaVA-1.5**。
	1. 浅层对齐方法在视觉语言模型（VLMs）中的性能主要挑战在于视觉数据和语言数据之间缺乏深度融合。浅层对齐方法之所以表现不佳，是因为它们依赖于“冻结”的语言模型权重，这些权重本质上是为了处理文本令牌而训练的。这导致了显著的不匹配问题，因为视觉特征在文本输入空间中没有直接对应。因此，当这些视觉特征经过多层转换时，往往会偏离深层语言模型层预期的输入分布。这种不对齐在图像字幕生成等任务中尤为明显，因为任务的特定性（如写作风格和字幕长度）只能通过浅层方法在视觉特征中进行表面编码。
	2. ![[Pasted image 20241203141127.png]]
2. 另外一种常见策略是在预训练或监督微调（SFT）阶段直接训练大型语言模型（LLM），**优化大语言模型的参数（Qwen-VL）**。
	1. 这种方法可能会影响模型的泛化能力，特别是在以文本输出为重点的任务中。传统上，大型语言模型是在大量仅含文本的数据集上进行预训练的（Raffel et al., 2020），这与图文对数据集（如 LAION 和 COYO）相比，数据分布有显著差异。这种转变常常导致灾难性遗忘，即模型在其原始领域的能力下降。
	2. ![[Pasted image 20241203141419.png]]
	3. 将一个拥有 80 亿参数的语言模型用于 VLM 预训练，可以导致自然语言生成（NLG）性能下降 87.3%（Driess et al., 2023）
	4. ![[Pasted image 20241203140345.png]]
在向大语言模型添加一流的视觉理解能力的同时，是否有可能保留其自然语言处理（NLP）能力？

## 本文贡献

1.  **CogVLM 模型**，它在保留预训练大型语言模型全部能力的同时，深度整合了视觉和语言特征。CogVLM-17B 从 Vicuna-7B 训练而来，在 17 个经典跨模态基准上达到了最新的性能水平。
	1. **视觉专家模块的有效性和深度融合的重要性**：在LLM中嵌入可学习权重，但不改变其NLP能力。

## CogVLM

![[Pasted image 20241203135519.png]]
$$
Attention(X, W_I, W_T)=softmax(\frac{Tril(QK^T)}{\sqrt{D}})V
$$
$$
Q = concat(XIW_I^Q, X_TW_T^Q)
$$
$$
K = concat(XIW_I^K, X_TW_T^K) 
$$
$$
V = concat(XIW_I^V, X_TW_T^V)
$$
$$
FFN(X)=concat(FFN_I(X_I), FFN_T(X_T))
$$
## 实验

### 图像标题提取
![[Pasted image 20241203140647.png]]

### VQA&LVLM
![[Pasted image 20241203140848.png]]

### VG（REC）

![[Pasted image 20241203140905.png]]

### 消融实验

![[Pasted image 20241203140936.png]]