1. Referring Expression Comprehension, REC
2. Phrase Grounding, PG

**类别**： 
- **经典视觉定位**：根据给定的文本描述，在图像中定位特定区域。
- **短语定位**：识别并定位文本短语中提到的所有实体。
- **广义视觉定位**：不仅定位单个目标，还可以定位多个目标，甚至是没有目标的情况。

**核心任务**：根据给定的文本描述，在图像中定位特定的区域。

这一任务的目标是模拟人类在社交对话中的指代关系，赋予机器类似的多模态理解能力。[^1](https://arxiv.org/abs/2412.20206)

**近年的新任务**：
1. 基于预训练的视觉定位
2. 多模态大语言模型的视觉定位
3. [广义视觉定位](https://zhida.zhihu.com/search?content_id=252299059&content_type=Article&match_order=1&q=%E5%B9%BF%E4%B9%89%E8%A7%86%E8%A7%89%E5%AE%9A%E4%BD%8D&zhida_source=entity)
4. 千兆像素级别的视觉定位

**相关任务**：
1. 指代表达生成（REG）：REG任务旨在生成描述图像中特定区域的文本
2. 指代表达分割（RES）：RES任务则要求在图像中分割出与文本描述对应的区域。

**研究方法**：
1. 全监督
2. 弱监督
3. 半监督
4. 无监督
5. 零样本
6. 多任务
7. 广义视觉定位

## 全监督研究方法与数据集

### 按模型区分

#### CNN

#### Transformer

#### 基于视觉语言预训练（VLP）的迁移

#### 基于多模态大语言模型（GMLLMs）

### 按框架分类

#### **2+1结构**

**如TransVG，分别对视觉和语言进行独立编码，然后通过融合编码器进行跨模态特征融合**

#### 2+2结构

**如MDETR，基于DETR的编码器-解码器架构，通过查询锚点生成定位框。**

#### **双编码器结构**

**如TransVG++，直接丢弃融合模块，提高模型效率。**

#### **单塔结构**

**如OneRef，利用模态共享的特征空间，简化模型设计。**

#### **GMLLM结构**

**如Shikra、Ferret，将视觉信息映射到语言模型的特征空间，将视觉定位任务转化为自回归语言任务。**

![[Pasted image 20250822120503.png]]


### 相关数据集与基准测试

1. RefCOCO
2. RefCOCO+
3. RefCOCOg

## 弱监督研究方法

**[弱监督视觉定位](https://zhida.zhihu.com/search?content_id=252299059&content_type=Article&match_order=1&q=%E5%BC%B1%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E5%AE%9A%E4%BD%8D&zhida_source=entity)（WSVG）旨在减少对标注框的依赖，仅利用图像-文本对进行模型训练。**
### 基于提案的方法

1. **句子重建策略**：通过外部检测器生成区域提案，并重建整个查询文本，建立匹配和重建损失。代表性工作包括GroundR（2016）、KAC-Net（2018）等。
2. **对比学习**：通过构建正负样本对，计算InfoNCE损失。代表性工作包括CCL（2020）、NCE-Distill（2021）等。
3. **关系感知实例精炼**：利用语言句子结构和场景图进行目标区域精炼。代表性工作包括MATN（2018）、ReIR（2021）等。
4. **伪标签生成**：通过伪标签和定位图进行自训练。代表性工作包括CPL（2023）、g++（2023）等。

###  基于VLP的弱监督迁移

1. **VLP辅助的弱监督**：利用VLP模型（如CLIP、BLIP）的跨模态对齐能力增强提案-文本相似度计算。代表性工作包括RefCLIP（2023）、QueryMatch（2024）等。
2. **VLP模型的弱监督迁移**：通过VLP模型生成跨模态注意力图，并结合外部检测器进行弱监督定位。代表性工作包括ALBEF（2021）、X-VLM（2022）等。

## 半监督研究方法

**[半监督视觉定位](https://zhida.zhihu.com/search?content_id=252299059&content_type=Article&match_order=1&q=%E5%8D%8A%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E5%AE%9A%E4%BD%8D&zhida_source=entity)（SSVG）利用部分标注数据和未标注数据进行模型训练**

### 

[^1]: 
