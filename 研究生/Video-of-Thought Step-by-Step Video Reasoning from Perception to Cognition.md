- 本文介绍了一种新颖的视频思维（VoT）推理框架，用于复杂的视频理解和推理任务。

- 它提出了 MotionEpic，一种新的视频多模态大语言模型 (MLLM)，集成了细粒度时空场景图 (STSG) 表示。

- VoT 框架将复杂的视频推理任务分解为更简单的子问题，从低级像素感知到高级认知解释逐步解决它们。

- 现有的视频理解模型难以处理复杂的视频，缺乏细粒度的时空感知理解和认知层面的理解

- MotionEpic 通过集成 STSG 表示实现细粒度像素级时空视频接地 23。

- VoT 框架遵循五步推理过程： A.任务定义和目标识别 b.物体追踪 C.动作分析 D.通过排名回答问题 E.答案验证

总结：
 1. 写了一个Python脚本用于从视频中提取等间隔的图片。基于之前训练过的YOLOv8检测器，检测图片中的对象位置和类别保存到标签文件，共得到了1041张电学实验图片。后续需要检查每一张图片和对应的标签，对标签进行微调，保证正确。
 2. 阅读“Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition”论文，本文应对当前视频理解领域存在的细粒度的时空感知理解和认知级视频场景理解两大瓶颈，提出多模态大预言模型（MLLM）MotionEpic和VoT推理框架。MotionEpic以文本（包含针对视频的提问等）、视频、和时空场景图（STSG）作为输入（以STSG的格式凝聚问题回答的关键信息），得到文本响应和STSG，其中经过指令调优，MotionEpic可以自动生成后续的STSG（后续不需要认为提供STSG），STSG中包含对视频中关键对象的跟踪信息。在结构上MotionEpic包含一个视频Encoder和Projector、 STSG Encoder和一个大语言模型（LLM）（它基于大语言模型产生跟踪结果），在具体的实验中本文使用Vicuna-7B作为LLM，ViT-L/14作为视频Encoder，Q-Former作为Projector，就Graph Transformer实现STSG Encoder。VoT框架包含5个阶段，其核心思想为将原始问题分解为一系列更小且细粒度更高的问题，通过解决小问题帮助解决原始问题。在第一阶段VoT得到问题定义，选择出视频中和问题高度相关的对象。第二阶段以STSG的格式对感兴趣的对象和对线间的关系进行跟踪。第三阶段基于之前的所有信息分析关键对象之间的交互动作。第四阶段从待选项（第一阶段中有待选项的输入）中选择最可能的原始问题的答案。第5阶段验证答案的正确性和合理性。本文描述的任务场景和我们的任务有相似之处，两者都对视频中的场景中的细粒度感知有所要求。我访问了该文章的官网和GitHub，项目代码还在TBD状态。